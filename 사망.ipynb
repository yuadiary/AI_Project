{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "사망.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFddGNPlynNr"
      },
      "source": [
        "## AI 악성코드 탐지 프로젝트\n",
        "\n",
        "팀명: 사망\n",
        "\n",
        "팀원: 임준범(20182010)\n",
        "\n",
        "소감: 결과물이 매우 부족하지만 그래도 튜토리얼 코드를 어느정도 이해하고 csv 출력까지는 성공할 수 있었습니다. 제 실력의 부족함을 실감했고 AI 악성코드 탐지가 어떻게 돌아가는지 이해하는 데 도움이 되었습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXVCdF1erDdX"
      },
      "source": [
        "!pip install pefile\n",
        "!pip install numpy\n",
        "!pip install sklearn\n",
        "!pip install lightgbm\n",
        "!pip install tqdm\n",
        "!pip install pandas\n",
        "!pip install graphviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qdHY2XIorPm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlPmGzNzoxk4"
      },
      "source": [
        "!unzip /content/drive/MyDrive/데이터.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG6dU3v5d535"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pprint\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import RFE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZQx0Zu1d539"
      },
      "source": [
        "SEED = 41\n",
        "\n",
        "def read_label_csv(path):\n",
        "    label_table = dict()\n",
        "    with open(path, \"r\", encoding=\"cp949\") as f:\n",
        "        for line in f.readlines()[1:]:\n",
        "            fname, label = line.strip().split(\",\")\n",
        "            label_table[fname] = int(label)\n",
        "    return label_table\n",
        "\n",
        "def read_json(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_model(**kwargs):\n",
        "    if kwargs[\"model\"] == \"rf\":\n",
        "        return RandomForestClassifier(random_state=kwargs[\"random_state\"], n_jobs=4)\n",
        "    elif kwargs[\"model\"] == \"dt\":\n",
        "        return DecisionTreeClassifier(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"lgb\":\n",
        "        return LGBMClassifier(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"svm\":\n",
        "        return SVC(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"lr\":\n",
        "        return LogisticRegression(random_state=kwargs[\"random_state\"], n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"knn\":\n",
        "        return KNeighborsClassifier(n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"adaboost\":\n",
        "        return AdaBoostClassifier(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"mlp\":\n",
        "        return MLPClassifier(random_state=kwargs[\"random_state\"])\n",
        "    else:\n",
        "        print(\"Unsupported Algorithm\")\n",
        "        return None\n",
        "    \n",
        "\n",
        "def train(X_train, y_train, model):\n",
        "    '''\n",
        "        머신러닝 모델을 선택하여 학습을 진행하는 함수\n",
        "\t\n",
        "        :param X_train: 학습할 2차원 리스트 특징벡터\n",
        "        :param y_train: 학습할 1차원 리스트 레이블 벡터\n",
        "        :param model: 문자열, 선택할 머신러닝 알고리즘\n",
        "        :return: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    clf = load_model(model=model, random_state=SEED)\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def evaluate(X_test, y_test, model):\n",
        "    '''\n",
        "        학습된 머신러닝 모델로 검증 데이터를 검증하는 함수\n",
        "\t\n",
        "        :param X_test: 검증할 2차원 리스트 특징 벡터\n",
        "        :param y_test: 검증할 1차원 리스트 레이블 벡터\n",
        "        :param model: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    predict = model.predict(X_test)\n",
        "    print(\"정확도\", model.score(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bkPrtk0d539"
      },
      "source": [
        "## 레이블 테이블 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvMGI6Abd53-"
      },
      "source": [
        "label_1 = read_label_csv(\"학습데이터_정답.csv\")\n",
        "label_2 = read_label_csv(\"검증데이터_정답.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_5flwONd53-"
      },
      "source": [
        "## 특징 벡터 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJtHyLDZd53_"
      },
      "source": [
        "class PeminerParser:\n",
        "    def __init__(self, path):\n",
        "        self.report = read_json(path)\n",
        "        self.vector = []\n",
        "    \n",
        "    def process_report(self):  \n",
        "        self.vector = [value for _, value in sorted(self.report.items(), key=lambda x: x[0])]\n",
        "        return self.vector\n",
        "    \n",
        "\n",
        "class EmberParser:\n",
        "    def __init__(self, path):\n",
        "        self.report = read_json(path)\n",
        "        self.vector = []\n",
        "    \n",
        "    def get_histogram_info(self):\n",
        "        histogram = np.array(self.report[\"histogram\"])\n",
        "        total = histogram.sum()\n",
        "        vector = histogram / total\n",
        "        return vector.tolist()\n",
        "    \n",
        "    def get_string_info(self):\n",
        "        strings = self.report[\"strings\"]\n",
        "\n",
        "        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0\n",
        "        vector = [\n",
        "            strings['numstrings'], \n",
        "            strings['avlength'], \n",
        "            strings['printables'],\n",
        "            strings['entropy'], \n",
        "            strings['paths'], \n",
        "            strings['urls'],\n",
        "            strings['registry'], \n",
        "            strings['MZ']\n",
        "        ]\n",
        "        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()\n",
        "        return vector\n",
        "    \n",
        "    def get_general_file_info(self):\n",
        "        general = self.report[\"general\"]\n",
        "        vector = [\n",
        "            general['size'], general['vsize'], general['has_debug'], general['exports'], general['imports'],\n",
        "            general['has_relocations'], general['has_resources'], general['has_signature'], general['has_tls'],\n",
        "            general['symbols']\n",
        "        ]\n",
        "        return vector\n",
        "\n",
        "    def process_report(self):\n",
        "        vector = []\n",
        "        vector += self.get_general_file_info()\n",
        "        vector += self.get_histogram_info()\n",
        "        vector += self.get_string_info()\n",
        "        return vector\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zfzfUI9d53_"
      },
      "source": [
        "## 학습데이터 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmjFZ-sFd53_"
      },
      "source": [
        "# 데이터의 특징 벡터 모음(2차원 리스트) : X\n",
        "# 데이터의 레이블 모음(1차원 리스트) : y\n",
        "X1, y1 = [], []\n",
        "files = os.listdir(\"EMBER/학습데이터\") + os.listdir(\"PEMINER/학습데이터\")\n",
        "for fname in files:\n",
        "    feature_vector = []\n",
        "    label = label_1[fname.replace('.json','')]\n",
        "    for data in [\"PEMINER/학습데이터\", \"EMBER/학습데이터\"]:\n",
        "        path = f\"{data}/{fname}\"\n",
        "        if data == \"PEMINER/학습데이터\":\n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        else:\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        \n",
        "    X1.append(feature_vector)\n",
        "    y1.append(label)\n",
        "\n",
        "np.asarray(X1).shape, np.asarray(y1).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YafOZYVEw4Oz"
      },
      "source": [
        "## 검증데이터 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch_RhG7Mw3yp"
      },
      "source": [
        "X2, y2 = [], []\n",
        "files = os.listdir(\"EMBER/검증데이터\")\n",
        "for fname in files:\n",
        "    feature_vector = []\n",
        "    label = label_2[fname.replace('.json','')]\n",
        "    for data in [\"PEMINER/검증데이터\",  \"EMBER/검증데이터\"]:\n",
        "        path = f\"{data}/{fname}\"\n",
        "        if data == \"PEMINER/검증데이터\":\n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        else:\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        \n",
        "    X2.append(feature_vector)\n",
        "    y2.append(label)\n",
        "\n",
        "np.asarray(X2).shape, np.asarray(y2).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTCCcycjd54B"
      },
      "source": [
        "## 학습 및 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU7jlt_P57St"
      },
      "source": [
        "# 학습\n",
        "models = []\n",
        "for model in [\"lgb\"]:\n",
        "    clf = train(X1, y1, model)\n",
        "    models.append(clf)\n",
        "\n",
        "\n",
        "# 검증\n",
        "# 실제 검증 시에는 제공한 검증데이터를 검증에 사용해야 함\n",
        "for model in models:\n",
        "    evaluate(X2, y2, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvsjy4WO5XIb"
      },
      "source": [
        "## 테스트데이터 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86ZoADXj5ZjI"
      },
      "source": [
        "X3 = []\n",
        "Tfiles = os.listdir(\"EMBER/테스트데이터\")\n",
        "for fname in Tfiles:\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER/테스트데이터\", \"EMBER/테스트데이터\"]:\n",
        "        path = f\"{data}/{fname}\"\n",
        "        if data == \"PEMINER/테스트데이터\":\n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        else:\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        \n",
        "    X3.append(feature_vector)\n",
        "\n",
        "np.asarray(X3).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCkocE0cd54C"
      },
      "source": [
        "## csv로 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQkhayk5d54C"
      },
      "source": [
        "def csv_result(X, model):   \n",
        "    predicts = model.predict(X)\n",
        "    with open('predict.csv', 'w', encoding='cp949') as f:\n",
        "        wr = csv.writer(f)\n",
        "        wr.writerow(['file','predict'])\n",
        "        for i in range(len(X)):\n",
        "            wr.writerow([Tfiles[i].replace('.json',''), predicts[i]])\n",
        "            #print([Tfiles[i].replace('.json',''),predicts[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYJv2h7UrIVo"
      },
      "source": [
        "csv_result(X3, models[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}